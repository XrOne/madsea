Application de génération de séquences visuelles à partir de storyboards – Stack et Architecture
Contexte et Objectif du Projet
L’objectif est de développer une application (exécutable en local, avec relais possible sur le cloud) capable de transformer un storyboard en une séquence visuelle animée stylisée. L’utilisateur (réalisateur, storyboarder, etc.) pourra importer un storyboard au format PDF ou images contenant des illustrations de scènes avec leurs annotations (type de plan, ambiance, mouvements de caméra, dialogues, etc.). L’application devra alors :
Analyser le storyboard importé : extraire chaque scène/panel, y compris l’image de référence et le texte d’instruction ou de description associé (via OCR si nécessaire).

Générer automatiquement des images clés par scène en s’appuyant sur un style visuel pré-entraîné (exemple : style « silhouette cinématographique Déclic »). Ces images doivent respecter la composition du storyboard (poses, cadrages) et l’ambiance décrite.

Permettre l’entraînement et l’ajout de styles personnalisés : l’utilisateur peut fournir un jeu d’images de référence pour fine-tuner un modèle (via LoRA, fine-tuning DALL·E ou autre technique) et ainsi créer un nouveau style applicables aux scènes.

Appliquer les styles aux séquences : une fois un style sélectionné (soit un modèle pré-entraîné fourni, ex. « Déclic – silhouette cinématographique », soit un style personnalisé par l’utilisateur), l’application peut régénérer toutes les images de la séquence dans ce style. Les générations pourront utiliser divers moteurs (ex. diffusion local via ComfyUI, ou appels API type DALL·E, Midjourney, Stable Diffusion 2.1 (WAN 2.1), Runway, etc.).

Générer une séquence animée à partir des images clés : selon les cas, cela peut impliquer de faire évoluer les images dans le temps (mouvements de caméra, transitions entre plans, animations des éléments) via des modèles image-to-video ou text-to-video. L’application pourrait soit créer un animatic (suite d’images fixes montées avec fondus/mouvements de caméra simulés), soit exploiter des modèles d’IA vidéo pour générer de courtes animations pour chaque plan.

Architecture hybride local/cloud : intégrer une passerelle intelligente permettant d’exploiter des ressources cloud (ex. API de Runway ML pour génération vidéo, API Midjourney ou DALL·E en ligne) lorsque la puissance locale est insuffisante, tout en offrant un fallback local (exécution locale via ComfyUI + modèles LoRA optimisés) si possible. L’outil Ollama pourrait être utilisé pour déployer localement certains modèles de langage ou de vision si nécessaire, sans connexion externe.

Le résultat attendu est une séquence animée dans un style cohérent, exportable vers des outils de montage ou d’affinage (tel que Kling AI ou autre moteur d’animation) pour la post-production.

(Stack technique, architecture modulaire, modules, gestion des styles, génération vidéo, etc. – voir contenu complet du cahier des charges fourni précédemment pour tous les détails et la suite du texte.)
